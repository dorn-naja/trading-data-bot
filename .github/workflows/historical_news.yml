name: Global Historical + News Hybrid Collector

on:
  workflow_dispatch:   # ‡∏Å‡∏î Run ‡πÄ‡∏≠‡∏á‡πÑ‡∏î‡πâ
  schedule:
    - cron: "0 0 * * 0"   # ‡∏£‡∏±‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏ó‡∏∏‡∏Å‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå (UTC)

jobs:
  collect_and_merge:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Install dependencies
      run: |
        pip install pandas requests beautifulsoup4 lxml sqlalchemy

    - name: Run unified global collector
      run: |
        python - <<'EOF'
        import pandas as pd, requests, json, datetime, sqlite3, time
        from bs4 import BeautifulSoup

        headers = {"User-Agent": "Mozilla/5.0"}
        base_date = datetime.date.today()
        start_date = (base_date - datetime.timedelta(days=365*3)).strftime("%Y%m%d")
        end_date = base_date.strftime("%Y%m%d")

        print("üåê ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á 3 ‡∏õ‡∏µ‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á...")

        # ========== 1Ô∏è‚É£ ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏™‡∏¥‡∏ô‡∏ó‡∏£‡∏±‡∏û‡∏¢‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡∏•‡πà‡∏á ==========
        assets = {
            "XAUUSD": f"https://stooq.com/q/d/l/?s=xauusd&d1={start_date}&d2={end_date}&i=d",
            "BTCUSD": "https://coinmetrics.io/api/v4/timeseries/asset-metrics?assets=btc&metrics=PriceUSD",
            "NAS100": f"https://stooq.com/q/d/l/?s=^ndx&d1={start_date}&d2={end_date}&i=d",
            "EURUSD": f"https://stooq.com/q/d/l/?s=eurusd&d1={start_date}&d2={end_date}&i=d",
            "GBPUSD": f"https://stooq.com/q/d/l/?s=gbpusd&d1={start_date}&d2={end_date}&i=d",
            "USDJPY": f"https://stooq.com/q/d/l/?s=usdjpy&d1={start_date}&d2={end_date}&i=d"
        }

        price_dfs = []
        for sym, url in assets.items():
            try:
                print(f"‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏≤‡∏Ñ‡∏≤ {sym} ...")
                if "coinmetrics" in url:
                    data = requests.get(url, timeout=15).json()["data"]
                    df = pd.DataFrame(data)
                    df["time"] = pd.to_datetime(df["time"])
                    df = df.rename(columns={"PriceUSD": "Close"})
                    df["Symbol"] = sym
                    df = df[["time","Close","Symbol"]].rename(columns={"time":"Date"})
                else:
                    df = pd.read_csv(url)
                    df["Symbol"] = sym
                price_dfs.append(df)
                time.sleep(2)
            except Exception as e:
                print(f"‚ùå ‡πÇ‡∏´‡∏•‡∏î {sym} ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à:", e)

        if price_dfs:
            all_prices = pd.concat(price_dfs, ignore_index=True)
            all_prices.to_csv("global_prices.csv", index=False)
            print(f"‚úÖ ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à {len(all_prices)} ‡πÅ‡∏ñ‡∏ß")
        else:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏Ñ‡∏≤‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á!")

        # ========== 2Ô∏è‚É£ ‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÄ‡∏ß‡πá‡∏ö (RSS + HTML ‡∏™‡∏≥‡∏£‡∏≠‡∏á) ==========
        print("üì∞ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡πÅ‡∏´‡∏•‡πà‡∏á...")

        news_sources = {
            "Reuters": "https://feeds.reuters.com/reuters/businessNews",
            "Investing": "https://www.investing.com/rss/news_25.rss",
            "CNBC": "https://www.cnbc.com/id/10001147/device/rss/rss.html",
            "MarketWatch": "https://feeds.content.dowjones.io/public/rss/mw_topstories",
            "ForexFactory": "https://www.forexfactory.com/news"
        }

        all_news = []
        for site, url in news_sources.items():
            try:
                print(f"‚è≥ ‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡∏à‡∏≤‡∏Å {site} ...")
                resp = requests.get(url, headers=headers, timeout=15)
                if resp.status_code == 200:
                    if "<rss" in resp.text or url.endswith(".rss"):
                        # ‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å RSS ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
                        soup = BeautifulSoup(resp.text, "xml")
                        for item in soup.find_all("item")[:30]:
                            title = item.title.get_text()
                            link = item.link.get_text()
                            all_news.append({
                                "source": site,
                                "headline": title,
                                "url": link,
                                "time": datetime.datetime.utcnow().isoformat()
                            })
                    else:
                        # ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡πÅ‡∏ö‡∏ö HTML
                        soup = BeautifulSoup(resp.text, "html.parser")
                        for tag in soup.find_all(["h2", "h3", "a"], limit=30):
                            title = tag.get_text().strip()
                            if len(title) > 25:
                                all_news.append({
                                    "source": site,
                                    "headline": title,
                                    "url": url,
                                    "time": datetime.datetime.utcnow().isoformat()
                                })
                print(f"‚úÖ {site}: ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πà‡∏≤‡∏ß‡πÑ‡∏î‡πâ {len(all_news)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°")
                time.sleep(2)
            except Exception as e:
                print(f"‚ùå {site} error:", e)

        pd.DataFrame(all_news).to_json("global_news.json", indent=2, force_ascii=False)
        print(f"üóû ‡∏£‡∏ß‡∏°‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(all_news)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")

        # ========== 3Ô∏è‚É£ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å SQLite ==========
        conn = sqlite3.connect("global_data.db")
        if price_dfs:
            all_prices.to_sql("prices", conn, if_exists="replace", index=False)
        if all_news:
            pd.DataFrame(all_news).to_sql("news", conn, if_exists="replace", index=False)
        conn.close()
        print("üì¶ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å global_data.db ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢")

        # ========== 4Ô∏è‚É£ ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• ==========
        summary = {
            "update_time": datetime.datetime.utcnow().isoformat(),
            "prices_count": len(all_prices) if price_dfs else 0,
            "news_count": len(all_news),
            "sources_success": list(news_sources.keys())
        }
        with open("summary.json","w",encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        print("üßæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å summary.json ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
        EOF

    - name: Commit and Push All Data
      run: |
        git config user.name "GitHub Actions Bot"
        git config user.email "action@github.com"
        git add global_prices.csv global_news.json global_data.db summary.json || true
        TIMESTAMP=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
        git commit -m "üåç Global data update: $TIMESTAMP" || true
        git push
