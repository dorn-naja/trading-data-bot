name: Historical + News Hybrid Collector

on:
  workflow_dispatch:    # ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏î Run ‡∏î‡πâ‡∏ß‡∏¢‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤ Actions

jobs:
  collect_and_upload:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      # ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ actions/checkout@v3 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á repo ‡πÑ‡∏î‡πâ
      uses: actions/checkout@v3

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.10"

    - name: Install dependencies
      run: |
        # ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
        pip install requests beautifulsoup4 pandas sqlalchemy

    - name: Run historical data collector
      run: |
        python - <<'EOF'
        import requests, pandas as pd, datetime, sqlite3, json
        from bs4 import BeautifulSoup

        headers = {"User-Agent": "Mozilla/5.0"}
        assets = {
            "XAUUSD": "https://stooq.com/q/d/l/?s=xauusd&i=d",
            "BTCUSD": "https://stooq.com/q/d/l/?s=btcusd&i=d",
            "NAS100": "https://stooq.com/q/d/l/?s=^ndx&i=d",
            "EURUSD": "https://stooq.com/q/d/l/?s=eurusd&i=d"
        }

        print("üìä ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏Ñ‡∏≤‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á...")

        dfs = []
        for sym, url in assets.items():
            try:
                df = pd.read_csv(url)
                df["symbol"] = sym
                dfs.append(df)
            except Exception as e:
                print("‚ùå Error fetching", sym, e)

        all_data = pd.concat(dfs)
        conn = sqlite3.connect("historical_data.db")
        all_data.to_sql("history", conn, if_exists="replace", index=False)
        conn.close()
        print("‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å historical_data.db ‡πÅ‡∏•‡πâ‡∏ß")

        print("üì∞ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à...")
        news_list = []

        try:
            html = requests.get("https://www.reuters.com/markets/", headers=headers).text
            soup = BeautifulSoup(html, "html.parser")
            for a in soup.find_all("a", href=True)[:20]:
                t = a.get_text().strip()
                if len(t) > 20:
                    news_list.append({
                        "source": "Reuters",
                        "headline": t,
                        "url": "https://www.reuters.com" + a["href"]
                    })
        except Exception as e:
            print("Reuters error:", e)

        try:
            html = requests.get("https://www.forexfactory.com/news", headers=headers).text
            soup = BeautifulSoup(html, "html.parser")
            for a in soup.find_all("a", href=True)[:20]:
                t = a.get_text().strip()
                if len(t) > 20:
                    news_list.append({
                        "source": "ForexFactory",
                        "headline": t,
                        "url": "https://www.forexfactory.com" + a["href"]
                    })
        except Exception as e:
            print("ForexFactory error:", e)

        pd.DataFrame(news_list).to_json("news_archive.json", indent=2, force_ascii=False)
        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(news_list)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏•‡∏á news_archive.json ‡πÅ‡∏•‡πâ‡∏ß")
        EOF

    - name: Commit and Save Data to GitHub Repository
      run: |
        git config user.name "GitHub Actions Bot"
        git config user.email "action@github.com"
        
        git add historical_data.db news_archive.json
        TIMESTAMP=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
        git commit -m "Data Update: $TIMESTAMP" || true
        git push
